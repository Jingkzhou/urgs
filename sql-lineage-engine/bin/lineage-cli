import argparse
import sys
import json
import os
import uuid
import logging
import signal
from datetime import datetime
import time
# Updated imports for new structure
from parsers.sql_parser import LineageParser
from exporters.neo4j import Neo4jClient
from config.settings import settings

# Configure logging
class CustomFormatter(logging.Formatter):
    def format(self, record):
        timestamp = self.formatTime(record, "%Y-%m-%d %H:%M:%S")
        ms = int(record.msecs)
        return f"[{timestamp}.{ms:03d}] [{record.levelname}] [{record.name}] - {record.getMessage()}"

def setup_logging():
    root = logging.getLogger()
    # Remove existing handlers
    for h in root.handlers[:]:
        root.removeHandler(h)
    
    handler = logging.StreamHandler(sys.stderr)
    handler.setFormatter(CustomFormatter())
    root.addHandler(handler)
    root.setLevel(logging.INFO)

setup_logging()
logger = logging.getLogger("Engine")

# Supported SQL file extensions (case-insensitive)
SQL_EXTENSIONS = ('.sql', '.prc', '.pkb', '.pks', '.plb', '.pls', '.fnc', '.trg', '.vw', '.ddl')

def is_sql_file(filename):
    """Check if a file is a SQL or stored procedure file based on extension."""
    return filename.lower().endswith(SQL_EXTENSIONS)

# Helper function to get parser instance
def get_parser():
    return LineageParser()

# Helper function to get neo4j client
def get_neo4j_client():
    return Neo4jClient()

# Modified parallel parser to include duration in result
def parse_single_file_with_timing(task_args):
    import time
    from parsers.parallel_parser import parse_single_file
    start = time.time()
    result = parse_single_file(task_args)
    result["duration"] = time.time() - start
    return result

# Global executor for signal handling
_executor = None
_shutting_down = False

def signal_handler(sig, frame):
    global _shutting_down
    if _shutting_down:
        # 防止重复触发
        logger.info("Signal handler already triggered, forcing exit...")
        os._exit(1)
    
    _shutting_down = True
    sig_name = signal.Signals(sig).name if hasattr(signal, 'Signals') else sig
    logger.info(f"Received {sig_name} signal, initiating graceful shutdown...")
    
    if _executor:
        logger.info("Terminating process pool executor and all worker processes...")
        try:
            # 方法1: 获取所有 worker 进程 PID 并强制杀死
            if hasattr(_executor, '_processes'):
                for pid in list(_executor._processes.keys()):
                    try:
                        os.kill(pid, signal.SIGKILL)
                        logger.info(f"Sent SIGKILL to worker process: {pid}")
                    except ProcessLookupError:
                        logger.debug(f"Worker process {pid} already terminated")
                    except Exception as e:
                        logger.warning(f"Failed to kill worker process {pid}: {e}")
            
            # 方法2: 调用 shutdown
            try:
                _executor.shutdown(wait=False, cancel_futures=True)
                logger.info("Executor shutdown initiated")
            except Exception as e:
                logger.warning(f"Error during executor shutdown: {e}")
                
        except Exception as e:
            logger.error(f"Error during cleanup: {e}")
    
    logger.info("Cleanup complete, exiting...")
    # 使用 os._exit 确保立即退出，不会被其他清理代码阻塞
    os._exit(0)

def main():
    # 注册信号处理器，确保能正确响应终止请求
    signal.signal(signal.SIGTERM, signal_handler)
    signal.signal(signal.SIGINT, signal_handler)
    global _executor
    parser = argparse.ArgumentParser(description="SQL Lineage Engine CLI")
    subparsers = parser.add_subparsers(dest="command", help="Available commands")

    # Command: parse-sql
    sql_parser_cmd = subparsers.add_parser("parse-sql", help="Parse SQL and extract lineage")
    group = sql_parser_cmd.add_mutually_exclusive_group(required=True)
    group.add_argument("--sql", help="SQL string to parse")
    group.add_argument("--file", help="Path to SQL file to parse")
    sql_parser_cmd.add_argument("--dialect", default="mysql", help="SQL dialect (default: mysql)")
    sql_parser_cmd.add_argument("--output", choices=["csv", "json", "neo4j"], default="neo4j", help="Output mode (default: neo4j)")
    sql_parser_cmd.add_argument("--output-file", help="Path to write output to (CSV or JSON)")
    sql_parser_cmd.add_argument("--no-clear", action="store_true", help="Do not clear existing lineage data before parsing (default: clear all data)")
    
    # Metadata arguments
    sql_parser_cmd.add_argument("--version-id", help="Explicit version ID")
    sql_parser_cmd.add_argument("--repo-id", help="Git Repository ID")
    sql_parser_cmd.add_argument("--commit-sha", help="Git Commit SHA")
    sql_parser_cmd.add_argument("--ref", help="Git Ref")
    sql_parser_cmd.add_argument("--repo-root", help="Root directory of the repo (for calculating relative paths)")
    sql_parser_cmd.add_argument("--default-user", help="Default user/schema name for unqualified tables")
    
    args = parser.parse_args()

    if args.command == "parse-sql":
        start_time_all = datetime.now()
        logger.info(f"Engine starting... Command: {args.command}")
        logger.info(f"Configuration: dialect={args.dialect}, output={args.output}, no-clear={args.no_clear}")

        # lineage_parser instantiation DEFERRED to avoid JVM start in main process
        neo4j = get_neo4j_client()
        
        # 确保 Neo4j 索引存在（优化查询性能）
        if neo4j:
            logger.info("Checking Neo4j indexes...")
            try:
                neo4j.ensure_indexes()
            except Exception as e:
                logger.warning(f"Failed to ensure indexes (continuing anyway): {e}")
        
        import csv
        
        # 自动生成版本号
        if args.version_id:
            version_id = args.version_id
            logger.info(f"Using provided version ID: {version_id}")
        else:
            version_id = f"v{datetime.now().strftime('%Y%m%d_%H%M%S')}_{uuid.uuid4().hex[:6]}"
            logger.info(f"Generated lineage version record: {version_id}")
        
        if args.file and not os.path.exists(args.file):
            logger.error(f"Input file or directory not found: {args.file}")
            return

        # Prepare list of relative file paths for clearing
        relative_files = []
        if args.file and args.repo_root:
            abs_repo_root = os.path.abspath(args.repo_root)
            if os.path.isfile(args.file):
                 abs_file = os.path.abspath(args.file)
                 if abs_file.startswith(abs_repo_root):
                     relative_files.append(os.path.relpath(abs_file, abs_repo_root))
            elif os.path.isdir(args.file):
                 for root, dirs, files in os.walk(args.file):
                     for file in files:
                         if is_sql_file(file):
                             abs_file = os.path.abspath(os.path.join(root, file))
                             if abs_file.startswith(abs_repo_root):
                                 relative_files.append(os.path.relpath(abs_file, abs_repo_root))
        
        # Clear data logic
        if args.output == "neo4j" and neo4j and not args.no_clear:
             if args.repo_id and relative_files:
                 logger.info(f"Clearing lineage for repo {args.repo_id} and specific files...")
                 try:
                     neo4j.clear_lineage_by_repo_files(args.repo_id, relative_files)
                 except Exception as e:
                     logger.error(f"Failed to clear repo lineage data: {e}")
             else:
                 logger.warning("No repo-id or files provided, skipping specific clear. Use --no-clear to suppress this if intended.")
                 # Fallback to old behavior if needed, or just do nothing safely
                 # logger.info("Clearing existing lineage data from Neo4j...")
                 # try:
                 #     neo4j.clear_all_lineage_data()
                 #     logger.info("Clearance completed.")
                 # except Exception as e:
                 #     logger.error(f"Failed to clear Neo4j data: {e}")

        # Determine output stream
        output_stream = sys.stdout
        file_handle = None
        
        if args.output_file:
             try:
                 file_handle = open(args.output_file, 'w', encoding='utf-8', newline='')
                 output_stream = file_handle
                 logger.info(f"Output redirected to file: {args.output_file}")
             except Exception as e:
                 logger.error(f"Error opening output file: {e}")
                 return

        # Track if CSV header has been printed
        csv_header_printed = False
        csv_writer = csv.writer(output_stream)

        def process_sql_content(content, parser, file_path=None):
            nonlocal csv_header_printed
            
            # For table lineage
            result = parser.parse(content, source_file=file_path)
            
            # For column lineage (with dependency_type and source_file)
            col_dependencies = parser.get_column_lineage(content, source_file=file_path)
            
            if args.output == "json":
                # Add column dependencies to the result for JSON output
                result["columnDependencies"] = col_dependencies
                # Print to output stream
                print(json.dumps(result, indent=2, ensure_ascii=False), file=output_stream)
            
            elif args.output == "csv":
                if not csv_header_printed:
                    csv_writer.writerow(["SourceTable", "SourceColumn", "TargetTable", "TargetColumn", "Type"])
                    csv_header_printed = True
                
                # Check for relationships key (new format)
                if "relationships" in result and result["relationships"]:
                    for rel in result["relationships"]:
                        csv_writer.writerow([
                            rel.get("source", ""),
                            "", # Source Column (not in table relations)
                            rel.get("target", ""),
                            "", # Target Column
                            rel.get("type", "")
                        ])
                else:
                    # Fallback to old cross-product logic if no relationships found (or old parser version)
                    # Table level (no column info)
                    if "sources" in result and "targets" in result:
                         for source in result["sources"]:
                            for target in result["targets"]:
                                 csv_writer.writerow([source, "", target, "", "fdd"])

                # Column level (with dependency_type)
                for dep in col_dependencies:
                     csv_writer.writerow([
                         dep.get("source_table", ""),
                         dep.get("source_column", ""),
                         dep.get("target_table", ""),
                         dep.get("target_column", ""),
                         dep.get("dependency_type", "fdd")  # 使用 dependency_type
                     ])
                     
            elif args.output == "neo4j":
                if neo4j:
                    inserted_any = False
                    
                    # Store relationships if available
                    if "relationships" in result and result["relationships"]:
                        for rel in result["relationships"]:
                            try:
                                neo4j.create_lineage(rel.get("source"), rel.get("target"))
                                inserted_any = True
                            except Exception as e:
                                logger.error(f"Error storing table lineage: {e}")
                    
                    # Fallback to old logic if no relationships found but sources/targets exist
                    if not inserted_any and "sources" in result and "targets" in result:
                        for source in result["sources"]:
                            for target in result["targets"]:
                                try:
                                    neo4j.create_lineage(source, target)
                                except Exception as e:
                                    logger.error(f"Error storing table lineage: {e}")
                    
                    # Batch insert column lineage with version
                    try:
                        neo4j.create_column_lineage_v2(col_dependencies, version=version_id, repo_id=args.repo_id)
                    except Exception as e:
                        logger.error(f"Error storing column lineage: {e}")

                else:
                    logger.warning("Neo4j client not initialized, skipping storage.")

        try:
            # 创建版本节点 (仅在 neo4j 模式下)
            if args.output == "neo4j" and neo4j:
                source_dir = args.file if args.file and os.path.isdir(args.file) else (
                    os.path.dirname(args.file) if args.file else None
                )
                neo4j.create_lineage_version(
                    version_id=version_id,
                    repo_id=args.repo_id,
                    commit_sha=args.commit_sha,
                    ref=args.ref,
                    source_directory=source_dir,
                    description=f"Parsed from {args.file or 'inline SQL'}"
                )
            
            if args.sql:
                lineage_parser = LineageParser(dialect=args.dialect, default_schema=args.default_user)
                process_sql_content(args.sql, parser=lineage_parser, file_path=None)
            elif args.file:
                if os.path.isdir(args.file):
                    # Directory mode - 使用多进程并行解析
                    from concurrent.futures import ProcessPoolExecutor, as_completed
                    from parsers.parallel_parser import prepare_file_tasks
                    
                    sql_files = []
                    for root, dirs, files in os.walk(args.file):
                        for file in files:
                            if is_sql_file(file):
                                sql_files.append(os.path.join(root, file))
                    
                    total_files = len(sql_files)
                    logger.info(f"Discovered {total_files} SQL files in directory: {args.file}")
                    
                    if total_files == 0:
                        logger.warning("No SQL files found in the specified directory.")
                    else:
                        # 准备任务参数
                        tasks = prepare_file_tasks(sql_files, args.dialect, default_schema=args.default_user)
                        
                        # 确定进程数
                        env_workers = os.environ.get("LINEAGE_MAX_WORKERS")
                        if env_workers and env_workers.isdigit():
                            max_workers = int(env_workers)
                        else:
                            max_workers = min(os.cpu_count() or 4, total_files)
                        logger.info(f"Initializing concurrent parser with {max_workers} processes (ProcessPoolExecutor)")
                        
                        # 收集所有结果
                        all_relationships = []
                        all_column_deps = []
                        success_count = 0
                        error_count = 0
                        
                        
                        _executor = ProcessPoolExecutor(max_workers=max_workers)
                        with _executor as executor:
                            # 提交所有任务
                            future_to_file = {executor.submit(parse_single_file_with_timing, task): task[0] for task in tasks}
                            
                            # 处理并行完成的任务
                            for idx, future in enumerate(as_completed(future_to_file), 1):
                                file_path = future_to_file[future]
                                f_name = os.path.basename(file_path)
                                progress = f"{idx}/{total_files} ({idx*100/total_files:.1f}%)"
                                
                                try:
                                    result = future.result()
                                    duration = result.get("duration", 0)
                                    if result["success"]:
                                        success_count += 1
                                        all_relationships.extend(result.get("relationships", []))
                                        all_column_deps.extend(result.get("column_dependencies", []))
                                        logger.info(f"[{progress}] Processed {f_name} - OK ({duration:.2f}s)")
                                    else:
                                        error_count += 1
                                        logger.error(f"[{progress}] Processed {f_name} - FAILED ({duration:.2f}s): {result['error']}")
                                except Exception as e:
                                    error_count += 1
                                    logger.exception(f"[{progress}] Process error for {f_name}: {e}")
                            
                        logger.info(f"Parsing cycle complete. Results: Success={success_count}, Failed={error_count}")
                        
                        # 批量写入结果
                        if args.output == "neo4j" and neo4j:
                            logger.info(f"Syncing results to Neo4j... ({len(all_relationships)} tables, {len(all_column_deps)} columns)")
                            try:
                                start_sync = time.time()
                                neo4j.create_lineage_batch(all_relationships)
                                neo4j.create_column_lineage_v2(all_column_deps, version=version_id, repo_id=args.repo_id)
                                logger.info(f"Sync completed in {time.time() - start_sync:.2f}s")
                            except Exception as e:
                                logger.error(f"Sync failed: {e}")
                        
                        elif args.output == "json":
                            combined_result = {
                                "relationships": all_relationships,
                                "columnDependencies": all_column_deps
                            }
                            print(json.dumps(combined_result, indent=2, ensure_ascii=False), file=output_stream)
                        
                        elif args.output == "csv":
                            if not csv_header_printed:
                                csv_writer.writerow(["SourceTable", "SourceColumn", "TargetTable", "TargetColumn", "Type"])
                                csv_header_printed = True
                            for rel in all_relationships:
                                csv_writer.writerow([rel.get("source", ""), "", rel.get("target", ""), "", rel.get("type", "")])
                            for dep in all_column_deps:
                                csv_writer.writerow([
                                    dep.get("source_table", ""),
                                    dep.get("source_column", ""),
                                    dep.get("target_table", ""),
                                    dep.get("target_column", ""),
                                    dep.get("dependency_type", "fdd")
                                ] )
                else:
                    # Single file mode
                    start_single = time.time()
                    try:

                        logger.info(f"Processing single file: {args.file}")
                        lineage_parser = LineageParser(dialect=args.dialect, default_schema=args.default_user)
                        # 尝试多种编码读取文件
                        sql_content = None
                        encodings_to_try = ['utf-8', 'gbk', 'gb2312', 'gb18030', 'latin-1']
                        for encoding in encodings_to_try:
                            try:
                                with open(args.file, 'r', encoding=encoding) as f:
                                    sql_content = f.read()
                                break
                            except UnicodeDecodeError:
                                continue
                        
                        if sql_content is None:
                            raise ValueError(f"Failed to decode file with any of: {encodings_to_try}")
                        process_sql_content(sql_content, parser=lineage_parser, file_path=args.file)
                        duration = time.time() - start_single
                        logger.info(f"Processing completed in {duration:.2f}s")
                    except Exception as e:
                        logger.error(f"Error reading SQL file: {e}")
        finally:
            if file_handle:
                file_handle.close()
            
            total_duration = datetime.now() - start_time_all
            logger.info(f"Engine shutdown. Total execution time: {total_duration}")
            # 打印版本信息 (仅供人类阅读，前端会捕获 stderr)
            if args.output == "neo4j":
                logger.info(f"Lineage stored with version: {version_id}")


    else:
        parser.print_help()

if __name__ == "__main__":
    main()


